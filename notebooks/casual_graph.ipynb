{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create the Ground Truth Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "# If running in an environment where __file__ is not defined, manually set the path\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(os.path.join(project_dir, 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from causalnex.structure import StructureModel\n",
    "from causalnex.plots import plot_structure, NODE_STYLE, EDGE_STYLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'order_id', 'driver_id', 'driver_action', 'lat', 'lng',\n",
       "       'Trip Origin', 'Trip Destination', 'Trip Start Time', 'Trip End Time',\n",
       "       'trip_duration', 'Trip Origin Lat', 'Trip Origin Lng',\n",
       "       'Trip Destination Lat', 'Trip Destination Lng', 'weather', 'hour',\n",
       "       'day_of_week', 'is_weekend', 'Distance_km', 'Trip Duration Hours',\n",
       "       'Average Speed (km/h)', 'accepted_within_radius',\n",
       "       'unfulfilled_within_radius', 'origin_cluster', 'destination_cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../data/final_dataset.csv')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41011\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n"
     ]
    }
   ],
   "source": [
    "print(data['is_weekend'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thus we don't need is_weekend column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rain' 'no-rain']\n"
     ]
    }
   ],
   "source": [
    "print(data['weather'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "columns = ['id', 'order_id', 'driver_id', 'driver_action', 'lat', 'lng',\n",
    "       'Trip Origin', 'Trip Destination', 'Trip Start Time', 'Trip End Time',\n",
    "       'trip_duration', 'Trip Origin Lat', 'Trip Origin Lng',\n",
    "       'Trip Destination Lat', 'Trip Destination Lng', 'weather', 'hour',\n",
    "       'day_of_week', 'Distance_km', 'Trip Duration Hours',\n",
    "       'Average Speed (km/h)', 'accepted_within_radius',\n",
    "       'unfulfilled_within_radius', 'origin_cluster', 'destination_cluster']\n",
    "\n",
    "data = data[columns]\n",
    "\n",
    "#take only the first 1000 rows\n",
    "data = data.head(1000)\n",
    "# Convert boolean columns to integers\n",
    "# data['is_holiday'] = data['is_holiday'].astype(int)\n",
    "# data['is_weekend'] = data['is_weekend'].astype(int)\n",
    "# Map 'rain' to 1 and 'no rain' to 0\n",
    "data['weather'] = data['weather'].map({'rain': 1, 'no-rain': 0})\n",
    "\n",
    "# Convert the column to int type\n",
    "data['weather'] = data['weather'].astype(int)\n",
    "\n",
    "# Rename columns to remove spaces and special characters and this helps to create the StructureModel.\n",
    "data.rename(columns={\n",
    "    'Trip Origin Lat': 'Trip_Origin_Lat',\n",
    "    'Trip Origin Lng': 'Trip_Origin_Lng',\n",
    "    'Trip Destination Lat': 'Trip_Destination_Lat',\n",
    "    'Trip Destination Lng': 'Trip_Destination_Lng',\n",
    "    'Average Speed (km/h)': 'Average_Speed_kmph',\n",
    "    'Trip Duration Hours': 'Trip_Duration_Hours'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "# Split the data into training and hold-out sets\n",
    "train_data, holdout_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save training and holdout sets\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "holdout_data.to_csv('holdout_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       id  order_id  driver_id driver_action       lat       lng  \\\n",
      "29    40    392005     171165      rejected  6.564058  3.370044   \n",
      "535  546    392033     244161      rejected  6.544610  3.362076   \n",
      "695  706    392036     243614      rejected  6.433190  3.503240   \n",
      "557  568    392033     243990      rejected  6.524337  3.360462   \n",
      "836  847    392041     243581      rejected  6.600685  3.352155   \n",
      "..   ...       ...        ...           ...       ...       ...   \n",
      "106  117    392009     245531      rejected  6.661377  3.309885   \n",
      "270  281    392021     245662      rejected  6.605025  3.294141   \n",
      "860  871    392041     243476      rejected  6.594846  3.339450   \n",
      "435  446    392033     244240      rejected  6.517712  3.381011   \n",
      "102  113    392009     244102      rejected  6.651027  3.300026   \n",
      "\n",
      "                       Trip Origin             Trip Destination  \\\n",
      "29     6.565087699999999,3.3844415  6.499696300000001,3.3509075   \n",
      "535            6.5214112,3.3672892          6.6383753,3.3331197   \n",
      "695            6.4408899,3.5030541  6.443346099999999,3.4827817   \n",
      "557            6.5214112,3.3672892          6.6383753,3.3331197   \n",
      "836  6.593734640652,3.356087407916  6.6417587,3.317330999999999   \n",
      "..                             ...                          ...   \n",
      "106            6.6636484,3.3082058           6.6185421,3.301634   \n",
      "270            6.5943334,3.3007843  6.429098199999999,3.4238032   \n",
      "860  6.593734640652,3.356087407916  6.6417587,3.317330999999999   \n",
      "435            6.5214112,3.3672892          6.6383753,3.3331197   \n",
      "102            6.6636484,3.3082058           6.6185421,3.301634   \n",
      "\n",
      "         Trip Start Time        Trip End Time  ...  weather  hour  \\\n",
      "29   2021-07-01 10:53:36  2021-07-01 11:27:51  ...        1    10   \n",
      "535  2021-07-01 07:27:43  2021-07-01 07:58:24  ...        1     7   \n",
      "695  2021-07-01 07:48:30  2021-07-01 08:47:05  ...        0     7   \n",
      "557  2021-07-01 07:27:43  2021-07-01 07:58:24  ...        0     7   \n",
      "836  2021-07-01 07:48:21  2021-07-01 08:31:33  ...        1     7   \n",
      "..                   ...                  ...  ...      ...   ...   \n",
      "106  2021-07-01 06:39:51  2021-07-01 07:41:12  ...        1     6   \n",
      "270  2021-07-01 07:11:36  2021-07-01 10:01:46  ...        1     7   \n",
      "860  2021-07-01 07:48:21  2021-07-01 08:31:33  ...        1     7   \n",
      "435  2021-07-01 07:27:43  2021-07-01 07:58:24  ...        0     7   \n",
      "102  2021-07-01 06:39:51  2021-07-01 07:41:12  ...        1     6   \n",
      "\n",
      "     day_of_week  Distance_km  Trip_Duration_Hours  Average_Speed_kmph  \\\n",
      "29             3     0.135393             0.570833            0.092381   \n",
      "535            3     0.231299             0.511389            0.175061   \n",
      "695            3     0.030165             0.976389            0.010996   \n",
      "557            3     0.231299             0.511389            0.175061   \n",
      "836            3     0.112034             0.720000            0.059899   \n",
      "..           ...          ...                  ...                 ...   \n",
      "106            3     0.080051             1.022500            0.028854   \n",
      "270            3     0.398190             2.836111            0.050034   \n",
      "860            3     0.112034             0.720000            0.059899   \n",
      "435            3     0.231299             0.511389            0.175061   \n",
      "102            3     0.080051             1.022500            0.028854   \n",
      "\n",
      "     accepted_within_radius  unfulfilled_within_radius  origin_cluster  \\\n",
      "29                        0                          1               5   \n",
      "535                       0                          1               3   \n",
      "695                       0                          1               8   \n",
      "557                       0                          1               3   \n",
      "836                       0                          1               2   \n",
      "..                      ...                        ...             ...   \n",
      "106                       0                          1               7   \n",
      "270                       0                          1               7   \n",
      "860                       0                          1               2   \n",
      "435                       0                          1               3   \n",
      "102                       0                          1               7   \n",
      "\n",
      "     destination_cluster  \n",
      "29                     2  \n",
      "535                    3  \n",
      "695                    6  \n",
      "557                    3  \n",
      "836                    3  \n",
      "..                   ...  \n",
      "106                    8  \n",
      "270                    1  \n",
      "860                    3  \n",
      "435                    3  \n",
      "102                    8  \n",
      "\n",
      "[800 rows x 25 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from causalnex.structure import StructureModel\n",
    "from causalnex.structure.notears import from_pandas\n",
    "# from causalnex.structure.notears import from_pandas_dynamic\n",
    "from causalnex.plots import plot_structure, NODE_STYLE, EDGE_STYLE\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Define the columns to include in the causal graph\n",
    "columns = ['trip_duration', 'Trip_Origin_Lat', 'Trip_Origin_Lng',\n",
    "           'Trip_Destination_Lat', 'Trip_Destination_Lng', 'Distance_km', 'weather',\n",
    "           'Average_Speed_kmph', 'accepted_within_radius', 'unfulfilled_within_radius']\n",
    "\n",
    "# Use the selected columns\n",
    "train_data = train_data[columns]\n",
    "\n",
    "# Create the ground truth causal graph using the NOTEARS algorithm\n",
    "ground_truth_sm = from_pandas(train_data, w_threshold=0.80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"ground_truth.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x775ba7951810>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Plot the causal graph\n",
    "viz = plot_structure(\n",
    "    ground_truth_sm,\n",
    "    all_node_attributes=NODE_STYLE.WEAK,\n",
    "    all_edge_attributes=EDGE_STYLE.WEAK\n",
    ")\n",
    "\n",
    "viz.show('ground_truth.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct parents of the target variable: ['trip_duration', 'Average_Speed_kmph', 'accepted_within_radius']\n"
     ]
    }
   ],
   "source": [
    "# Function to get parents of a node in a StructureModel\n",
    "def get_parents(structure_model, node):\n",
    "    return [edge[0] for edge in structure_model.edges if edge[1] == node]\n",
    "\n",
    "# Identify direct parents of the target variable in the stable causal graph\n",
    "target_variable = 'unfulfilled_within_radius'\n",
    "direct_parents = get_parents(ground_truth_sm, target_variable)\n",
    "print(\"Direct parents of the target variable:\", direct_parents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2 .Create Graphs with Increasing Fractions of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction_10.html\n",
      "fraction_30.html\n",
      "fraction_50.html\n",
      "fraction_70.html\n",
      "fraction_90.html\n"
     ]
    }
   ],
   "source": [
    "# Define fractions to use\n",
    "fractions = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "# Store the structure models\n",
    "fraction_smodels = []\n",
    "\n",
    "for frac in fractions:\n",
    "    # Sample a fraction of the data\n",
    "    sample_data = train_data.sample(frac=frac, random_state=42)\n",
    "    \n",
    "    # Create a causal graph for the sample data\n",
    "    sm = from_pandas(sample_data, w_threshold=0.80)\n",
    "    fraction_smodels.append(sm)\n",
    "\n",
    "    # Optionally plot and save each graph\n",
    "    viz = plot_structure(\n",
    "        sm,\n",
    "        all_node_attributes=NODE_STYLE.WEAK,\n",
    "        all_edge_attributes=EDGE_STYLE.WEAK\n",
    "    )\n",
    "    viz.show(f'fraction_{int(frac*100)}.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compare Graphs Using Jaccard Similarity Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction: 10%, Jaccard Similarity Index: 0.25\n",
      "Fraction: 30%, Jaccard Similarity Index: 0.19\n",
      "Fraction: 50%, Jaccard Similarity Index: 0.36\n",
      "Fraction: 70%, Jaccard Similarity Index: 0.43\n",
      "Fraction: 90%, Jaccard Similarity Index: 0.68\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(graph1, graph2):\n",
    "    \"\"\"Calculate the Jaccard similarity index between two graphs.\"\"\"\n",
    "    edges1 = set(graph1.edges)\n",
    "    edges2 = set(graph2.edges)\n",
    "    intersection = edges1.intersection(edges2)\n",
    "    union = edges1.union(edges2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "# Calculate Jaccard Similarity Index for each fraction graph\n",
    "similarity_scores = []\n",
    "\n",
    "for sm in fraction_smodels:\n",
    "    score = jaccard_similarity(ground_truth_sm, sm)\n",
    "    similarity_scores.append(score)\n",
    "\n",
    "# Print the similarity scores\n",
    "for frac, score in zip(fractions, similarity_scores):\n",
    "    print(f'Fraction: {frac*100:.0f}%, Jaccard Similarity Index: {score:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the Jaccard Similarity Index Scores\n",
    "\n",
    "* Fraction: 10%, Jaccard Similarity Index: 0.25\n",
    "    Interpretation: When using only 10% of the data, the resulting causal graph shares 25% of its edges with the ground truth graph. This indicates some similarity but also significant differences, which is expected due to the limited amount of data.\n",
    "\n",
    "* Fraction: 30%, Jaccard Similarity Index: 0.19\n",
    "    Interpretation: Using 30% of the data results in a Jaccard Similarity Index of 0.19, which is lower than that of the 10% fraction. This suggests that the causal graph generated from 30% of the data is less similar to the ground truth graph compared to the one generated from 10%. This could be due to random variations in the data or sampling differences.\n",
    "\n",
    "* Fraction: 50%, Jaccard Similarity Index: 0.36\n",
    "    Interpretation: With 50% of the data, the similarity increases to 0.36. This indicates that as more data is used, the resulting causal graph becomes more similar to the ground truth graph. However, there is still a significant amount of variation.\n",
    "\n",
    "* Fraction: 70%, Jaccard Similarity Index: 0.43\n",
    "    Interpretation: Using 70% of the data yields a Jaccard Similarity Index of 0.43. The similarity continues to improve, indicating that the graph generated from 70% of the data is more similar to the ground truth graph compared to the graphs generated from smaller fractions.\n",
    "\n",
    "* Fraction: 90%, Jaccard Similarity Index: 0.68\n",
    "    Interpretation: At 90% of the data, the Jaccard Similarity Index is 0.68. This high similarity score indicates that the causal graph generated from 90% of the data closely resembles the ground truth graph. This suggests that with 90% of the data, most of the significant causal relationships identified in the full dataset are captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct parents of the target variable: ['trip_duration', 'Average_Speed_kmph', 'accepted_within_radius']\n"
     ]
    }
   ],
   "source": [
    "# Function to get parents of a node in a StructureModel\n",
    "def get_parents(structure_model, node):\n",
    "    return [edge[0] for edge in structure_model.edges if edge[1] == node]\n",
    "\n",
    "# Identify direct parents of the target variable in the stable causal graph\n",
    "target_variable = 'unfulfilled_within_radius'\n",
    "direct_parents = get_parents(ground_truth_sm, target_variable)\n",
    "print(\"Direct parents of the target variable:\", direct_parents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 1\n",
      "Components: [{'weather', 'Average_Speed_kmph', 'accepted_within_radius', 'Trip_Destination_Lng', 'Trip_Destination_Lat', 'Distance_km', 'Trip_Origin_Lat', 'trip_duration', 'Trip_Origin_Lng', 'unfulfilled_within_radius'}]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Identify the weakly connected components\n",
    "components = list(nx.weakly_connected_components(ground_truth_sm))\n",
    "print(f\"Number of components: {len(components)}\")\n",
    "print(\"Components:\", components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalnex.network import BayesianNetwork\n",
    "\n",
    "# Convert the StructureModel to a BayesianNetwork\n",
    "bn = BayesianNetwork(ground_truth_sm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit CPDs to the BayesianNetwork\n",
    "bn = bn.fit_node_states_and_cpds(train_data, method=\"BayesianEstimator\", bayes_prior=\"K2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalnex.inference import InferenceEngine\n",
    "\n",
    "# Create an inference engine for the Bayesian Network\n",
    "ie = InferenceEngine(bn)\n",
    "\n",
    "# Perform the intervention for each question\n",
    "\n",
    "# Perform the intervention (assuming 1km additional distance)\n",
    "new_distance = train_data['Distance_km'] + 1\n",
    "# Question 1: Impact of Drivers Moving 1km Every 30 Minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "states =ie.query()[\"Distance_km\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0603554321200701: 0.0, 0.0696208262878737: 0.0, 0.0800506237784369: 0.0, 0.090254529052517: 0.0, 0.1353929437399915: 0.0, 0.1748434791092629: 0.0, 0.1750598193309729: 0.0, 0.231299362311575: 0.0, 0.3318390285569496: 0.0, 0.3981903956773348: 0.0, 0.438444003609278: 0.0, 0.4898543873915751: 0.0, 0.5501764892977402: 0.9999999999999969}\n"
     ]
    }
   ],
   "source": [
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new distance array with the 1km increase\n",
    "increased_distances = train_data['Distance_km'] + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'states' is a dictionary representing discretized distance bins after causal model creation\n",
    "num_bins = len(states)\n",
    "\n",
    "intervention_value = {}\n",
    "for i in range(num_bins):\n",
    "    current_state = list(states.keys())[i]\n",
    "\n",
    "    # Intervention strategy: Uniform shift of probability mass to the next bin (adjust as needed)\n",
    "    if i == num_bins - 1:\n",
    "        intervention_value[current_state] = 1.0  # Last bin absorbs remaining probability\n",
    "    else:\n",
    "        intervention_value[current_state] = 0.0  # Probability in this bin moves to the next\n",
    "        intervention_value[list(states.keys())[i + 1]] = 1.0  # Probability goes to next bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform the intervention (assuming 'ie' is your causal inference engine)\n",
    "ie.do_intervention(\"Distance_km\", intervention_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect on unfulfilled requests with 1km movement every 30 minutes: {0: 0.027363184079602115, 1: 0.9726368159203977}\n"
     ]
    }
   ],
   "source": [
    "states =ie.query()[\"Distance_km\"]\n",
    "# Create a new distance array with the 1km increase\n",
    "increased_distances = train_data['Distance_km'] + 1\n",
    "\n",
    "\n",
    "# Assuming 'states' is a dictionary representing discretized distance bins after causal model creation\n",
    "num_bins = len(states)\n",
    "\n",
    "intervention_value = {}\n",
    "for i in range(num_bins):\n",
    "    current_state = list(states.keys())[i]\n",
    "\n",
    "    # Intervention strategy: Uniform shift of probability mass to the next bin (adjust as needed)\n",
    "    if i == num_bins - 1:\n",
    "        intervention_value[current_state] = 1.0  # Last bin absorbs remaining probability\n",
    "    else:\n",
    "        intervention_value[current_state] = 0.0  # Probability in this bin moves to the next\n",
    "        intervention_value[list(states.keys())[i + 1]] = 1.0  # Probability goes to next bin\n",
    "\n",
    "# Perform the intervention (assuming 'ie' is your causal inference engine)\n",
    "ie.do_intervention(\"Distance_km\", intervention_value)\n",
    "\n",
    "# Query the effect on the number of unfulfilled requests\n",
    "result = ie.query()['unfulfilled_within_radius']\n",
    "print(\"Effect on unfulfilled requests with 1km movement every 30 minutes:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "* The intervention of moving drivers 1km every 30 minutes results in a very high probability (97.26%) that there will still be unfulfilled requests within the specified radius. * Conversely, there is a low probability (2.74%) that there will be no unfulfilled requests.\n",
    "\n",
    "* Implications\n",
    "\n",
    "    - High Probability of Unfulfilled Requests: The high probability of unfulfilled requests remaining after the intervention suggests that simply moving drivers 1km every 30 minutes is not sufficient to significantly reduce the number of unfulfilled requests. Other factors might need to be considered or additional interventions may be necessary to achieve a more substantial impact.\n",
    "\n",
    "    - Further Analysis Needed: This insight indicates the need for further analysis to identify other potential factors or more effective interventions that could reduce the number of unfulfilled requests more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Knowing Location of Next 20% of Orders within 5km Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Improved accuracy in order location prediction\n",
    "states_origin_lat = ie.query()[\"Trip_Origin_Lat\"]\n",
    "states_origin_lng = ie.query()[\"Trip_Origin_Lng\"]\n",
    "states_destination_lat = ie.query()[\"Trip_Destination_Lat\"]\n",
    "states_destination_lng = ie.query()[\"Trip_Destination_Lng\"]\n",
    "\n",
    "# Example intervention values, improving accuracy\n",
    "intervention_value_origin_lat = {state: 1/len(states_origin_lat) for state in states_origin_lat}\n",
    "intervention_value_origin_lng = {state: 1/len(states_origin_lng) for state in states_origin_lng}\n",
    "intervention_value_destination_lat = {state: 1/len(states_destination_lat) for state in states_destination_lat}\n",
    "intervention_value_destination_lng = {state: 1/len(states_destination_lng) for state in states_destination_lng}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the interventions\n",
    "ie.do_intervention(\"Trip_Origin_Lat\", intervention_value_origin_lat)\n",
    "ie.do_intervention(\"Trip_Origin_Lng\", intervention_value_origin_lng)\n",
    "ie.do_intervention(\"Trip_Destination_Lat\", intervention_value_destination_lat)\n",
    "ie.do_intervention(\"Trip_Destination_Lng\", intervention_value_destination_lng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect on unfulfilled requests with improved order location accuracy: {0: 0.027363184079602327, 1: 0.9726368159204085}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query the effect on the number of unfulfilled requests\n",
    "result = ie.query()['unfulfilled_within_radius']\n",
    "print(\"Effect on unfulfilled requests with improved order location accuracy:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
